wf := import("@platforma-sdk/workflow-tengo:workflow")
ll := import("@platforma-sdk/workflow-tengo:ll")
file := import("@platforma-sdk/workflow-tengo:file")
exec := import("@platforma-sdk/workflow-tengo:exec")
assets:= import("@platforma-sdk/workflow-tengo:assets")
maps:= import("@platforma-sdk/workflow-tengo:maps")
xsv := import("@platforma-sdk/workflow-tengo:pframes.xsv")
pframes := import("@platforma-sdk/workflow-tengo:pframes")
pSpec := import("@platforma-sdk/workflow-tengo:pframes.spec")
pt := import("@platforma-sdk/workflow-tengo:pt")
path := import("@platforma-sdk/workflow-tengo:path")
json := import("json")
text := import("text")
render := import("@platforma-sdk/workflow-tengo:render")
strings := import("@platforma-sdk/workflow-tengo:strings")
runAlignmentTpl := assets.importTemplate(":run-alignment")

prepareFastaSw := assets.importSoftware("@platforma-open/milaboratories.immune-assay-data.prepare-fasta:main")
addHeaderSw := assets.importSoftware("@platforma-open/milaboratories.immune-assay-data.add-header:main")
covModeCalcSw := assets.importSoftware("@platforma-open/milaboratories.immune-assay-data.coverage-mode-calc:main")

wf.prepare(func(args){
	bundleBuilder := wf.createPBundleBuilder()
	bundleBuilder.addAnchor("main", args.datasetRef) 
	bundleBuilder.addSingle(args.targetRef)
	return {
		columns: bundleBuilder.build()
	}
})

prepareAssayFile := func(args, file, xsvType) {
	// assign ids to assay sequences
	ptw := pt.workflow()
	df := ptw.frame({
		file: file,
		xsvType: xsvType
	})

	//////// calculate sequence id ////////
	df = df.addColumns(
		pt.col(args.sequenceColumnHeader).hash("sha256", "base64_alphanumeric", 120).alias("seqId")
	)
	//////// add label to ids ////////
	df = df.withColumns(
        pt.col("seqId").
            strReplace("\\d", "", { replaceAll: true }).
            strSlice(0, 5).               // Take first 5 characters
            strToUpper().                 // Convert to uppercase
            alias("tmpLabel")
    )
	df = df.withColumns(
        pt.rank(pt.col("seqId")).  // Rank based on clonotypeKeyCol (default ascending)
            over(pt.col("tmpLabel")).   // Partition by prefixTempCol
            alias("rank")
    )
	df = df.withColumns(
        pt.when(pt.col("rank").gt(pt.lit(1))).
            then(pt.concatStr([pt.lit("A"), pt.col("tmpLabel"), pt.col("rank").cast("String")], { delimiter: "-" })).
            otherwise(pt.concatStr([pt.lit("A"), pt.col("tmpLabel")], { delimiter: "-" })).
            alias("seqIdLabel")
    )
	df = df.withoutColumns("rank", "tmpLabel")

	//////// add sequence column ////////
	df = df.addColumns(
		pt.col(args.sequenceColumnHeader).alias("sequence")
	)
	df.save("output.tsv")
	
	return ptw.run().getFile("output.tsv")
}

prepareClonesTsv := func(args) {
	columns := args.columns
	datasetSpec := columns.getSpec(args.datasetRef)

	cloneTable := pframes.tsvFileBuilder()
	
	cloneTable.setAxisHeader(datasetSpec.axesSpec[1].name, "seqId")
	cloneTable.add(columns.getColumn(args.targetRef), {header: "sequence"})

	cloneTable.mem("16GiB")
	cloneTable.cpu(1)
	return cloneTable.build()
}

/**
 * Convert tsv file to fasta file
 * @param fileTsv - tsv file
 * @return fasta file run result
 */
runTsvToFasta := func(fileTsv) {
	e := exec.builder().
		software(prepareFastaSw).
		mem("16GiB").
		cpu(1).
		addFile("input.tsv", fileTsv).
		arg("-i").arg("input.tsv").
		arg("-o").arg("output.fasta").
		arg("--seq_col").arg("sequence").
		arg("--id_col").arg("seqId").
		saveFile("output.fasta")

	return e.run()
}

assayColumnName := func(header) {
	return "pl7.app/vdj/assay-data/" + strings.substituteSpecialCharacters(header)
}

wf.body(func(args) {
	importFile := file.importFile(args.fileHandle)
	datasetSpec := args.columns.getSpec(args.datasetRef)
	targetSpec := args.columns.getSpec(args.targetRef)
	
	// aminoacid or nucleotide
	sequenceColumnInfo := undefined
	for col in args.importColumns {
		if col.header == args.sequenceColumnHeader {
			sequenceColumnInfo = col
			break
		}
	}

	// aminoacid or nucleotide
	targetSequenceType := targetSpec.domain["pl7.app/alphabet"]
	assaySequenceType := sequenceColumnInfo.sequenceType

	if targetSequenceType == undefined {
		ll.panic("Target sequence type is undefined")
	}
	
	if assaySequenceType == undefined {
		ll.panic("Assay sequence type is undefined")
	}

	handleUrl := ll.parseUrl(args.fileHandle)
	jsonPayload := handleUrl.Path[1:]
	fileInfo := json.decode(jsonPayload)

	fileName := ""
	if fileInfo.localPath != undefined {
		fileName = fileInfo.localPath
	} else if fileInfo.path != undefined {
		fileName = fileInfo.path
	} else {
		ll.panic("Could not determine filename from file handle: ", args.fileHandle)
	}

	fileNameParts := path.split(fileName, ".")
	xsvType := "tsv"
	if len(fileNameParts) > 1 {
		xsvType = fileNameParts[len(fileNameParts)-1]
	}

	assayTsv := prepareAssayFile(args, importFile.file, xsvType)
	clonesTsv := prepareClonesTsv(args)

	// prepare fasta
	clonesFastaRun := runTsvToFasta(clonesTsv)
	assayFastaRun := runTsvToFasta(assayTsv)
	clonesFasta := clonesFastaRun.getFile("output.fasta")
	assayFasta := assayFastaRun.getFile("output.fasta")

	// Dynamically determine coverage mode by comparing average sequence lengths
	coverageMode := exec.builder().
		software(covModeCalcSw).
		mem("16GiB").
		cpu(1).
		addFile("clones.fasta", clonesFasta).
		addFile("assay.fasta", assayFasta).
		arg("--clones-fasta").arg("clones.fasta").
		arg("--assay-fasta").arg("assay.fasta").
		arg("--output").arg("coverage_mode.txt").
		saveFileContent("coverage_mode.txt").
		run()

	covMode := coverageMode.getFileContent("coverage_mode.txt")

	mmseqsSearchType := "0"
	if targetSequenceType == "aminoacid" && assaySequenceType == "aminoacid" {
		//1: amino acid
		mmseqsSearchType = "1"
	} else if targetSequenceType == "nucleotide" && assaySequenceType == "nucleotide" {
		// 3: nucleotide
		mmseqsSearchType = "3"
	} else if targetSequenceType == "nucleotide" && assaySequenceType == "aminoacid" {
		// 4: translated nucleotide alignment
		mmseqsSearchType = "4"
	} else if targetSequenceType == "aminoacid" && assaySequenceType == "nucleotide"  {
		// 2: nucleotide
		mmseqsSearchType = "2"
	}

	runMmseqs := render.create(runAlignmentTpl, {
		covMode: covMode,
		mmseqsSearchType: mmseqsSearchType,
		coverageThreshold: args.settings.coverageThreshold,
		identityThreshold: args.settings.identity,
		similarityType: args.settings.similarityType,
		clonesFasta: clonesFasta,
		assayFasta: assayFasta
	})

	mmseqsOutput := runMmseqs.output("mmseqsOutput")

	// @TODO remove header stuff and replace with pt when available (!)
	addHeaderRunResult := exec.builder().
		software(addHeaderSw).
		mem("16GiB").
		cpu(1).
		arg("-i").arg("results.tsv").
		arg("-o").arg("results_with_header.tsv").
		addFile("results.tsv", mmseqsOutput).
		saveFile("results_with_header.tsv").
		run()

	mmseqsResultTsv := addHeaderRunResult.getFile("results_with_header.tsv")
	mmseqsResultTsvContent := addHeaderRunResult.getFileContent("results_with_header.tsv")

	emptyResults := len(text.trim_space(string(mmseqsResultTsvContent))) == 0
	blockId := wf.blockId().getDataAsJson()

	assayPframe := undefined
	epf := undefined

	if emptyResults {
		assayPframe = pframes.emptyPFrame()
		epf = pframes.emptyPFrame()
	} else {
		//////// Process tables ////////
		ptw := pt.workflow()
		df := ptw.frame({
			file: mmseqsResultTsv,
			xsvType: "tsv"
		})

		// Cast columns to ensure correct types for aggregation
		df = df.withColumns(
			pt.col("evalue").cast("Float64").alias("evalue"),
			pt.col("bits").cast("Float64").alias("bits")
		)

		cols := []
		for _, col in ["bits", "evalue", "target", "pident", "alnlen", "mismatch", "gapopen", "qstart", "qend", "tstart", "tend"] {
			cols = append(cols,
						pt.col(col).maxBy(
								pt.col("evalue").multiply(-1),
								pt.col("bits")
							).alias(col)
						)
		}
		
		df = df.groupBy("query").agg(cols...)
		df.save("results.tsv")

		// assay data import summary
		assayDf := ptw.frame({
			file: assayTsv,
			xsvType: "tsv"
		})
		// import how many matches per assay sequence found
		assayDf = assayDf.join(
			df.groupBy("target").agg(
				pt.col("query").count().alias("queryCount")
			),
			{
				how: "left",
				leftOn: "seqId",
				rightOn: "target"
			}
		)
		assayDf.save("assayData.tsv")

		// clones 
		clonesDf := df.join(assayDf,
			{
				how: "left",
				leftOn: "target",
				rightOn: "seqId"
			}
		)

		clonesDf.save("clonesData.tsv")
		ptw = ptw.run()
		
		//////// Building outputs & exports ////////
		assayColumns := [
			{
				column: "seqIdLabel",
				spec: {
					name: "pl7.app/label",
					valueType: "String",
					annotations: {
						"pl7.app/label": "Sequence Id",
						"pl7.app/table/fontFamily": "monospace"
					}
				}
			},	
			{
				column: "queryCount",
				spec: {
					name: "pl7.app/vdj/assay/queryCount",
					valueType: "Int",
					annotations: {
						"pl7.app/label": "Matched Clones",
						"pl7.app/table/orderPriority": "9000"
					}
				}
			},	
			{
				column: sequenceColumnInfo.header,
				id: strings.substituteSpecialCharacters(sequenceColumnInfo.header),
				spec: {
					name: "pl7.app/vdj/sequence",
					valueType: "String",
					domain: {
						"pl7.app/alphabet": assaySequenceType
					},
					annotations: {
						"pl7.app/label": sequenceColumnInfo.header,
						"pl7.app/table/fontFamily": "monospace",
						"pl7.app/table/orderPriority": "10000"
					}
				}
			}
		]
		
		columnsToImport := args.importColumns
		if args.selectedColumns != undefined && len(args.selectedColumns) > 0 {
			selectedHeaders := {}
			for header in args.selectedColumns {
				selectedHeaders[header] = true
			}
			
			filteredColumns := []
			for col in args.importColumns {
				// Always include the main sequence column
				if col.header == args.sequenceColumnHeader || selectedHeaders[col.header] {
					filteredColumns = append(filteredColumns, col)
				}
			}
			columnsToImport = filteredColumns
		}

		for h in columnsToImport {
			if h.header == args.sequenceColumnHeader {
				continue
			}
			assayColumns = append(assayColumns, {
				column: h.header,
				id: strings.substituteSpecialCharacters(h.header),
				spec: {
					name: assayColumnName(h.header),
					valueType: h.type,
					annotations: {	
						"pl7.app/label": h.header,
						"pl7.app/table/orderPriority": "1000"
					}
				}
			})
		}

		assayImportResults := xsv.importFile(ptw.getFile("assayData.tsv"), "tsv", {
			axes: [{
				column: "seqId",
				spec: {
					name: "pl7.app/vdj/assay/sequenceId",
					type: "String",
					domain: {
						"pl7.app/blockId": blockId
					},
					annotations: {
						"pl7.app/label": "Sequence Id",
						"pl7.app/table/fontFamily": "monospace"
					}
				}
			}],
			columns: assayColumns,
			annotations: {
				"pl7.app/isAnchor": "true"
			}
		}, {cpu: 1, mem: "16GiB"})

		// "bits", "evalue", "pident"
		cloneColumns := [
		{
			column: "seqIdLabel",
			spec: {
				name: "pl7.app/vdj/assay/sequenceIdLabel",
				valueType: "String",
				annotations: {
					"pl7.app/label": "Assay Sequence Id",
					"pl7.app/table/fontFamily": "monospace",
					"pl7.app/table/visibility": "optional"
				}
			}
		},
		{
			column: "bits",
			spec: {
				name: "pl7.app/alignment/bitScore",
				valueType: "Float",
				annotations: {
					"pl7.app/label": "Bit Score",
					"pl7.app/table/visibility": "optional"
				}
			}
		},
		{
			column: "evalue",
			spec: {
				name: "pl7.app/alignment/evalue",
				valueType: "Float",
				annotations: {
					"pl7.app/label": "E-value",
					"pl7.app/table/visibility": "optional"
				}
			}
		},
		{
			column: "pident",
			spec: {
				name: "pl7.app/alignment/pident",
				valueType: "Float",
				annotations: {
					"pl7.app/label": "Percentage of identical matches",
					"pl7.app/table/visibility": "optional"
				}
			}
		}]

		for h in columnsToImport {
			cloneColumns = append(cloneColumns, {
				column: h.header,
				id: strings.substituteSpecialCharacters(h.header),
				spec: {
					name: assayColumnName(h.header),
					valueType: h.type,
					annotations: {	
						"pl7.app/label": h.header,
						"pl7.app/table/visibility": h.header == args.sequenceColumnHeader ? "optional" : "default"
					}
				}
			})
		}

		// insert domain
		for col in cloneColumns {
			col.spec.domain = maps.deepMerge(col.spec.domain, {
				"pl7.app/blockId": blockId
			})
		}

		cloneImportResults := xsv.importFile(
			ptw.getFile("clonesData.tsv"), "tsv", {
				axes: [{
					column: "query",
					spec: datasetSpec.axesSpec[1]
				}],
				columns: cloneColumns
			},
			{ splitDataAndSpec: true, cpu: 1, mem: "16GiB" }
		)

		trace := pSpec.makeTrace(datasetSpec,
			{
				type: "milaboratories.immune-assay-data",
				importance: 30,
				label: "Assay Data"
			})

		epfB := pframes.pFrameBuilder()
		for k, v in cloneImportResults {
			epfB.add(k, trace.inject(v.spec), v.data)
		}
		epf = epfB.build()
		assayPframe = pframes.exportFrame(assayImportResults)
	}

	result := {
		outputs: {
			dataImportHandle: importFile.handle,
			table: assayPframe,
			mmseqsOutput: mmseqsOutput, // @TODO tmp fix to resolve CID conflicts
			emptyResults: emptyResults
		}
	}

	if !emptyResults {
		result.exports = {
			epf: epf
		}
	}

	return result
})
